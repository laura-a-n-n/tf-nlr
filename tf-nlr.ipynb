{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tf-nlr: Neural Lumigraph Rendering in TensorFlow 2 ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align:center'><img src='img/M2.gif' alt='Rendered gif of NLR scene M2.' /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook shows how to use `tf-nlr`, an implementation of the neural pipeline of \"Neural Lumigraph Rendering\" as described in [Kellnhofer et al. (2021)](https://arxiv.org/abs/2103.11571).\n",
    "\n",
    "To run this notebook, please download the free [NLR dataset](https://drive.google.com/file/d/1BBpIfrqwZNYmG1TiFljlCnwsmL2OUxNT/view) and extract it in a new folder called `data` in the root directory of `tf-nlr`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import datetime\n",
    "from argparse import Namespace\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import loss\n",
    "from model.nlr import NeuralLumigraph\n",
    "\n",
    "from lib.sphere_tracer import SphereTracer\n",
    "from lib.data import Data\n",
    "from lib.math import dot, sphere_data, gen_3d_noise, compute_gradients, normalize_vectors\n",
    "\n",
    "from train import train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - The `lib.data.Data` object packages data loading and ray generation.\n",
    " - Initialize a `Data` object with the command `Data('path/to/dataset', img_ratio)`.\n",
    " - Note that it must be bound to the `model.nlr.NeuralLumigraph` object with the `bind_data` method, as we show below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlr_data = Data('./data/nlr_dataset/L1', img_ratio=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlr_data.compute_rays(scene_radius_scale=.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlr_dataset = nlr_data.compute_dataset(v_img=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Create a neural lumigraph model ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='text-align:center'><img src='img/L1.gif' alt='Rendered gif of NLR scene L1.' style='width:120px;height:auto' /></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - With `model.nlr.NeuralLumigraph.pretrain`, you can initialize the neural SDF to a sphere of radius 0.5. If you don't want to, skip to [this cell](#train-from-scratch).\n",
    " - This requires some learning rate schedule or otherwise manual adjustment of the learning rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "nlr = NeuralLumigraph(omega=(30, 30), hidden_omega=(30, 30))\n",
    "nlr.compile(s_lr=tf.keras.optimizers.schedules.ExponentialDecay(1e-4, 1000, .5, staircase=True), e_lr=1e-4)\n",
    "nlr.pretrain(radius=.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also load a model already trained on NLR data. You'll want to pass `permute_inputs=True` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlr = NeuralLumigraph(omega=(30,30), hidden_omega=(30,30), permute_inputs=True)\n",
    "nlr.load_model('h5/L1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span id='train-from-scratch'>If you want to train from scratch, start here.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlr = NeuralLumigraph(s_h5='h5/pretrain/S.h5', omega=(30,30), hidden_omega=(30,30))\n",
    "nlr.compile(s_lr=tf.keras.optimizers.schedules.ExponentialDecay(1e-6, 30000, .5, staircase=True), \n",
    "            e_lr=tf.keras.optimizers.schedules.ExponentialDecay(1e-4, 30000, .5, staircase=True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate the sphere tracer ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - NLR uses a bidirectional sphere tracer that converges points which reach within a small threshold of zero.\n",
    " - Before the official code for [MetaNLR++](https://github.com/alexanderbergman7/metanlrpp) was released, back in July 2021, `tf-nlr` used a sphere tracer which more closely resembled that of [IDR](https://github.com/lioryariv/idr/blob/main/code/model/ray_tracing.py), which MetaNLR++ also appears to use. However, since the MetaNLR++ code contains the version of the sphere tracer used in the original NLR, we've since transferred over to a translation of their code. Our original sphere tracer can be found as the `alt_trace` method of the `lib.sphere_tracer.SphereTracer` object.\n",
    " - The `SphereTracer` object should be bound to the model with the `bind_tracer` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sphere_tracer = SphereTracer(sphere_trace_n=16)\n",
    "nlr.bind_tracer(sphere_tracer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bind data and render ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bind the data object as below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlr.bind_data(nlr_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Here are three examples of how to use a `NeuralLumigraph` object to render views.\n",
    " - The NLR object supports rendering RGB, depth, and normal maps.\n",
    " - By default, the `render` and `write_img` methods render the view indexed by the `v_img` attribute of the bound `Data` object. This `v_img` attribute is set to `-1`, i.e. the last image in the dataset, unless passed as a keyword argument into the `compute_dataset` method or otherwise set as an attribute to the `NeuralLumigraph` object. Alternatively, you can pass a `v_img` keyword argument to these methods to specify you want to render a different image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With `write_img`, you don't have to do any of the plotting work; just call the method. It returns a `matplotlib.pyplot` object, so you can call `show` on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rendered_view, _ = nlr.write_img(v_img=16, compute_depth_img=True, compute_normal_img=True, write_to_file=False)\n",
    "rendered_view.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " - Alternatively, you can use `render` to have more control over how the result is plotted.\n",
    " - Pass functions as `transform_rays_o` and `transform_rays_d` keyword arguments to synthesize novel views."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for img_number in range(-1, -len(nlr_data.img_tensors)-1, -1):\n",
    "    final_img = nlr.render(v_img=img_number)\n",
    "\n",
    "    f = plt.figure(figsize=(10,4))\n",
    "    plt.subplot(121)\n",
    "    plt.imshow(tf.reshape(final_img, nlr.data.img_tensors[img_number][0].shape))\n",
    "    plt.subplot(122)\n",
    "    plt.imshow(nlr_data.img_tensors[img_number][0])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    tf.print(f'Panning left... please wait! {i}/3')\n",
    "    final_img = nlr.render(v_img=16, transform_rays_o=lambda x : x + tf.constant([-.05*i, 0., 0.]), verbose=False)\n",
    "    plt.imshow(tf.reshape(final_img, nlr.data.img_tensors[16][0].shape))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard training loop ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NLR uses an initial learning rate of $1 \\times 10^{-4}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use these learning rates!\n",
    "nlr.sdf.optimizer.learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(1e-6, 30000, .5, staircase=True)\n",
    "nlr.e.optimizer.learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(1e-4, 30000, .5, staircase=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below are training and validation parameters; the default values here should work in most cases, but you may have to adjust the device options depending on your hardware constraints."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt = dict(\n",
    "    training = dict(\n",
    "        # general\n",
    "        epochs = 750, # train for this many epochs\n",
    "        batch_size = 50000, # split the data into batches of this size\n",
    "        shuffle = 0, # shuffle the data with this buffer size (0 means full dataset size)\n",
    "\n",
    "        nan_exception = True, # raise exception on loss NaN?\n",
    "        print_losses = True, # print losses every step?\n",
    "        print_times = True, # print times?\n",
    "        print_clear = True, # clear output every epoch?\n",
    "\n",
    "        # checkpoints\n",
    "        checkpoints = dict(\n",
    "            write_checkpoints = True, # save checkpoints?\n",
    "            checkpoint_steps = 0, # checkpoint every this many steps; if 0, checkpoint on validation\n",
    "            checkpoint_path = 'h5',\n",
    "        ),\n",
    "        \n",
    "        # TensorBoard\n",
    "        tensorboard = dict(\n",
    "            write_summaries=True, # write to TensorBoard?\n",
    "            losses=['l_r', 'l_m', 'l_e'], # which losses to write\n",
    "            log_path='logs', # TensorBoard log path\n",
    "        ),\n",
    "        \n",
    "        # mask loss hyperparameters\n",
    "        mask_loss = dict(\n",
    "            alpha_increase = 250, # increase alpha every this many epochs\n",
    "            alpha_ratio = 2., # multiply alpha by this value\n",
    "            alpha = 50., # initial alpha value\n",
    "            num_samples = 80, # number of samples along ray to find minimal SDF value\n",
    "            batch_sampling = True, # if true, does two batches for sampling\n",
    "        ),\n",
    "\n",
    "        # loss weights\n",
    "        loss_weights = dict(\n",
    "            w_e = 1e-1, # eikonal weight\n",
    "            w_m = 1e2, # mask loss weight\n",
    "            w_s = 1e-2, # angular linearization weight\n",
    "        ),\n",
    "\n",
    "        # device options\n",
    "        device = dict(\n",
    "            diff_sphere_tracing_device = '/gpu:0', # device for the differentiable sphere tracing step\n",
    "            get_features_device = '/gpu:0', # device for recomputing normals and getting feature vectors\n",
    "            appearance_forward_device = '/gpu:0', # device for forward-passing to E\n",
    "            sampling_device = '/gpu:0', # device for ray sampling\n",
    "            l_r_device = '/gpu:0', # device for color loss\n",
    "            l_s_device = '/gpu:0', # device for angular smoothness loss\n",
    "            l_e_device = '/cpu:0', # device for eikonal loss\n",
    "            l_m_device = '/gpu:0', # device for soft mask loss\n",
    "            optim_device = '/gpu:0', # device for optimizer step\n",
    "        ),\n",
    "\n",
    "        # validation parameters\n",
    "        validation = dict(\n",
    "            validate = True, # render validation image?\n",
    "            validation_step = 0, # which step to validate after?\n",
    "            validation_epochs = 1, # validate every this many epochs\n",
    "            compute_depth_img = True, # render depth map?\n",
    "            compute_normal_img = True, # render image with normal map?\n",
    "            verbose = True, # verbose validation?\n",
    "            validation_out_path = 'out.png', # save validation image to this path\n",
    "            ipy_show = True, # if True, calls matplotlib.pyplot.show instead of saving to file\n",
    "        ),\n",
    "    ),\n",
    "    \n",
    "    rendering = dict(\n",
    "        light_dir = [.3202674, -0.91123605, -0.25899315], # lighting direction for normal image; if None, return RGB normal map\n",
    "        normal_bias = 70, # brightness parameter for normal image\n",
    "    ),\n",
    ")\n",
    "\n",
    "opt = Namespace(**opt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's the main training loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "train(opt, nlr=nlr, notebook=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
